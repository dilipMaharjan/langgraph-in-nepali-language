{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b82293",
   "metadata": {},
   "source": [
    "# Components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bc1b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load environment variables from .env file\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684bf7a5",
   "metadata": {},
   "source": [
    "## langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133ab0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure the Groq LLM\n",
    "from langchain_groq import ChatGroq \n",
    "\n",
    "llm = ChatGroq(model_name=\"llama-3.1-8b-instant\",temperature=0.7)\n",
    "# uncomment to validate the LLM configuration\n",
    "# llm.invoke(\"What is the capital of Nepal?\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c9a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of messages format in LangGraph\n",
    "from langchain_core.messages import AIMessage,HumanMessage\n",
    "\n",
    "messages = [AIMessage(content=f'I am LangGraph Expert. I can help you with your queries related to LangGraph.',name='LLM')]\n",
    "messages.append(HumanMessage(content=f'What is LangGraph?',name='User'))\n",
    "\n",
    "#print the messages\n",
    "for message in messages:\n",
    "    message.pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c8ce5b",
   "metadata": {},
   "source": [
    "## Router and Tool\n",
    "### Router helps selecting next step based on the user query\n",
    "### Tool helps performing specific task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791a2562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substract(a:int,b:int)->int:\n",
    "    \"\"\"\n",
    "   Subsctract b from a and return the result.\n",
    "   Args:\n",
    "       a (int): The first number.\n",
    "       b (int): The second number.\n",
    "   Returns:\n",
    "       int: The result of a - b.\n",
    "    \"\"\"\n",
    "    return a-b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6776d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Binding tools \n",
    "tools=llm.bind_tools([substract])\n",
    "messages.append(AIMessage(content=f\"You are a helpful assistant.Do not assume the question, if you don't know the answer, just say it so\",name='LLM'))\n",
    "\n",
    "tool_call=tools.invoke(\"Can you subtract 10 from 20?\")\n",
    "print(tool_call.content) \n",
    "print(tool_call.tool_calls)\n",
    "\n",
    "\n",
    "llm_call=tools.invoke(\"How are you?\")\n",
    "print(llm_call.content)\n",
    "print(llm_call.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee32ce6",
   "metadata": {},
   "source": [
    "## Creating state\n",
    " - You may try to use data class or pydantic for state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e0f0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from typing import Annotated\n",
    "from langgraph.graph.message import AnyMessage,add_messages\n",
    "\n",
    "# annotated will show all the message in the list\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage],add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccee361",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_message=AIMessage(content=f\"What is the answer\",name='LLM')\n",
    "\n",
    "# function of reducer : append the ai_message to the messages\n",
    "add_messages(messages,ai_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1ed8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(state:State):\n",
    "    return {\"messages\":[tools.invoke(state[\"messages\"])]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435c6ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build State Graph\n",
    "from IPython.display import display,Image\n",
    "from langgraph.graph import StateGraph,START,END\n",
    "\n",
    "build_graph=StateGraph(State)\n",
    "\n",
    "build_graph.add_node(\"llm_call\",llm_call)  \n",
    "build_graph.add_edge(START,\"llm_call\")\n",
    "build_graph.add_edge(\"llm_call\",END)\n",
    "\n",
    "graph=build_graph.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6de541a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the graph\n",
    "messages=graph.invoke({\"messages\":\"What is 10 minus 2\"})\n",
    "\n",
    "for message in messages['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28367820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode,tools_condition\n",
    "\n",
    "build_graph=StateGraph(State)\n",
    "tool=[substract]\n",
    "\n",
    "#add nodes\n",
    "build_graph.add_node(\"llm_call\",llm_call)  \n",
    "build_graph.add_node(\"tools\",ToolNode(tool))\n",
    "\n",
    "#add edges\n",
    "build_graph.add_edge(START,\"llm_call\")\n",
    "build_graph.add_conditional_edges(\"llm_call\",tools_condition)\n",
    "build_graph.add_edge(\"tools\",END)\n",
    "\n",
    "graph=build_graph.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11e5753",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the graph\n",
    "messages=graph.invoke({\"messages\":\"What is 10 minus 2\"})\n",
    "\n",
    "for message in messages['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db9689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the graph\n",
    "messages=graph.invoke({\"messages\":\"What is the capital of the world?\"})\n",
    "\n",
    "for message in messages['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c9848d",
   "metadata": {},
   "source": [
    "## Multiple Tools Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4fe458",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import ArxivQueryRun,WikipediaQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper,WikipediaAPIWrapper \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767db919",
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_wrapper_api=ArxivAPIWrapper(top_k_results=2,doc_content_chars_max=500)\n",
    "arxiv=ArxivQueryRun(api_wrapper=arxiv_wrapper_api)\n",
    "print(arxiv.name)\n",
    "arxiv.invoke(\"Love thy neighbour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d1e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_wrapper_api=WikipediaAPIWrapper(top_k_results=1,doc_content_chars_max=500)\n",
    "wiki=WikipediaQueryRun(api_wrapper=wiki_wrapper_api)\n",
    "print(wiki.name)\n",
    "wiki.invoke(\"Who is Jesus Christ.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959d81d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tools list\n",
    "tools=[arxiv,wiki]\n",
    "\n",
    "#bind it with llm\n",
    "llm_with_tools=llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4628e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools.invoke([HumanMessage(content=f\"Who is Jesus Christ?\") ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e3a49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools.invoke([HumanMessage(content=f\"Any research on 'Love thy neighbour'\") ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49dfc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tools.invoke([HumanMessage(content=f\"Who are you?\") ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36874132",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_calling_llm(state:State):\n",
    "    return {\"messages\":[llm_with_tools.invoke(state[\"messages\"])]}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414c597f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Node\n",
    "graph = StateGraph(State)\n",
    "\n",
    "#add nodes\n",
    "graph.add_node(\"tool_calling_llm\",tool_calling_llm)  \n",
    "graph.add_node(\"tools\",ToolNode(tools))\n",
    "\n",
    "#add edges\n",
    "graph.add_edge(START,\"tool_calling_llm\")\n",
    "graph.add_conditional_edges(\"tool_calling_llm\",tools_condition)\n",
    "graph.add_edge(\"tools\",END)\n",
    "\n",
    "graph=graph.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e9c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=graph.invoke({\"messages\":HumanMessage(content=f\"Who is Jesus Christ?\")})\n",
    "for message in messages['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72b98f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=graph.invoke({\"messages\":HumanMessage(content=f\"Who are you?\")})\n",
    "for message in messages['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b0806",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=graph.invoke({\"messages\":HumanMessage(content=f\"Any research on existinence of Aliens?\")})\n",
    "for message in messages['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36521a7a",
   "metadata": {},
   "source": [
    "### ReAct Agent Architecture\n",
    "- Used to develop complex Agent\n",
    "- ReAct : General Agent architecure\n",
    "    - Act: The model based on specific input calls specific tool\n",
    "    - Observe: Passes tool output back to the model\n",
    "    - Reason: The model will reason based on the output response from the tool to make next step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f6de63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom functions\n",
    "def multiply(a:int, b:int)->int:\n",
    "    \"\"\"Multiplys two numbers\n",
    "\n",
    "    Args:\n",
    "        a (int): first number\n",
    "        b (int): second number\n",
    "\n",
    "    Returns:\n",
    "        int: result\n",
    "    \"\"\"\n",
    "    return a*b\n",
    "\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Adds a and b\"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f464e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tools list\n",
    "tools=[arxiv,wiki,multiply,add]\n",
    "\n",
    "#bind it with llm\n",
    "llm_with_tools=llm.bind_tools(tools)\n",
    "# llm_with_tools.invoke([HumanMessage(content=f\"Any research on existinence of Aliens?\") ])\n",
    "\n",
    "# llm_with_tools.invoke([HumanMessage(content=f\"Who is Nelson Mandela?\") ])\n",
    "# llm_with_tools.invoke([HumanMessage(content=f\"Multilply 2 by 3\") ])\n",
    "# llm_with_tools.invoke([HumanMessage(content=f\"Add 2 and 4\")])\n",
    "llm_with_tools.invoke([HumanMessage(content=f\"Who are you?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc00981",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage],add_messages]\n",
    "    \n",
    "def tool_calling_llm(state:State):\n",
    "    return {\"messages\":[llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "# Build graph\n",
    "graph = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "graph.add_node(\"tool_calling_llm\",tool_calling_llm)\n",
    "graph.add_node(\"tools\",ToolNode(tools))\n",
    "\n",
    "# Add edges        \n",
    "graph.add_edge(START,\"tool_calling_llm\")\n",
    "graph.add_conditional_edges(\"tool_calling_llm\",tools_condition)\n",
    "graph.add_edge(\"tools\",\"tool_calling_llm\")\n",
    "\n",
    "graph=graph.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277528fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=graph.invoke({\"messages\":HumanMessage(content=f\"Any research on existinence of Aliens then summarize in 1 sentence?\")})\n",
    "for message in messages['messages']:\n",
    "    message.pretty_print()\n",
    "\n",
    "messages=graph.invoke({\"messages\":HumanMessage(content=f\"Who are you?\")})\n",
    "for message in messages['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1e2aaf",
   "metadata": {},
   "source": [
    "### Memory\n",
    "\n",
    "- Can automatically save graph state after each step by using checkpointer\n",
    "- Built in persistence layer allows langgraph to pick up from the last state update\n",
    "- Easier checkpoint to use is MemorySaver, an in memory key-value store for Graph State\n",
    "- All we need to do is simple compile graph with a checkointer\n",
    "\n",
    "Documentation ref: https://langchain-ai.github.io/langgraph/concepts/persistence/#get-state-history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0b6e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=graph.invoke({\"messages\":HumanMessage(content=f\"Add 5 and 6.\")})\n",
    "for message in messages['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e20e6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=graph.invoke({\"messages\":HumanMessage(content=f\"Then add 6 to the result.If you lack context, just say so.\")})\n",
    "for message in messages['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84d49ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Build graph with memory\n",
    "graph_memory = StateGraph(State)\n",
    "\n",
    "# Add nodes\n",
    "graph_memory.add_node(\"tool_calling_llm\",tool_calling_llm)\n",
    "graph_memory.add_node(\"tools\",ToolNode(tools))\n",
    "\n",
    "# Add edges        \n",
    "graph_memory.add_edge(START,\"tool_calling_llm\")\n",
    "graph_memory.add_conditional_edges(\"tool_calling_llm\",tools_condition)\n",
    "graph_memory.add_edge(\"tools\",\"tool_calling_llm\")\n",
    "\n",
    "#Initialize Memory\n",
    "memory=MemorySaver()\n",
    "\n",
    "graph_memory=graph_memory.compile(checkpointer=memory)\n",
    "display(Image(graph_memory.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b008ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "messages=graph_memory.invoke({\"messages\":HumanMessage(content=f\"Add 5 and 6.\")},config=config)\n",
    "for message in messages['messages']:\n",
    "    message.pretty_print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ea8d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=graph_memory.invoke({\"messages\":HumanMessage(content=f\"Add 5 and 6.\")},config=config)\n",
    "for message in messages['messages']:\n",
    "    message.pretty_print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb32384",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=graph_memory.invoke({\"messages\":HumanMessage(content=f\"Add 6 to the output. If you can't do it, just say it so.\")},config=config)\n",
    "for message in messages['messages']:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10b7b14",
   "metadata": {},
   "source": [
    "## Streaming\n",
    " - stream()\n",
    "    \n",
    "      Synchronous streaming\n",
    "\n",
    " - astream()\n",
    "    \n",
    "      Asynchronous streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab7ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "for chunk in graph_memory.stream({\"messages\":HumanMessage(content=f\"Add 2 to the output. If you can't do it, just say it so.\")},config=config,stream_mode=\"updates\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2dee12",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in graph_memory.stream({\"messages\":HumanMessage(content=f\"Add 2 to the output. If you can't do it, just say it so.\")},config=config,stream_mode=\"values\"):\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a1b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "inputs = {\"messages\": [HumanMessage(content=\"Add 2 to the output. If you can't do it, just say it so.\")]}\n",
    "async for event in graph_memory.astream_events(inputs, config, version=\"v2\"):\n",
    "    print(event)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9684da11",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
